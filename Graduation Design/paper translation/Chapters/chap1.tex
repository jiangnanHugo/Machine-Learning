\chapter{简介}
最近在机器学习领域中，许多学者正致力于从无标签输入数据中学习获得优良的特征表示，以便于更高层次的任务，如分类。目前的解决方案主要通过非监督式学习算法\cite{11,8,18}贪心地预训练多层次的特征，每次只训练一层，来获得多层次的特征表示。通过如下方法选择每一层的设计参数：需要学习的特征数量，特征被处理的位置，以及如何将系统的输入与输出编码。在本文中，我们研究了在单层网络模型上针对上述不同的选择，采用不同的特征学习方法，并分析其影响效果。我们的研究结果表明，几个互相不干扰的关键点：白化，大量特征和密集的特征提取，对系统表现有着很重要的影响，是所有算法中的优势所在。即采用简单的算法和单层的特征，而不是更新算法模型架构，也可能通过合理调节这些因子来获得当前最好的系统表现。

很多特征学习算法的主要缺点是其高复杂性和时间复杂性。此外，许多算法需要需要仔细选择多个超参数像学习速率，动量，稀疏惩罚因子，权重衰减因子等等，这些因子需要通过交叉验证手段来挑选，从而急剧增加了系统的运行时间。虽然最近发表的算法一直在改善在NORB \cite{16} 和CIFAR-10 \cite{13} 数据集上面的性能表现，但系统中仍然存在其他因素能明显影响特征学习算法的系统性能。具体地说，有许多``元参数'' 能决定系统的网络架构，例如感受野的大小和隐藏节点（特征）的数量。在实践中，这些参数通常由于计算瓶颈限制而预选被确定。例如，考虑到算法的运行时间我们可能会使用最多的特征数量。然而在本文中，我们采用另一种策略：我们使用十分简单的学习算法并且仔细选择网络模型的参数，期望能获得更好的性能表现。如果（在很多情况下）特征数量越多会有更好的表现，那么我们就可以充分这些学习算法的简洁性和运算速度来挑选更多的特征数量。

为此，我们将在第三节介绍一个简单的特征学习框架，它把特征学习算法作为``黑匣子''模块使用。对于该``黑匣子''，我们已经实现了几个现成的非监督学习算法：稀疏自动编码器，系数限制玻尔兹曼机，k-means聚类和高斯混合模型。然后我们分析了在特征学习框架中几种不同的因素对系统的性能的影响，包括：
\begin{itemize}
  \item 白化，这是深度学习中常用的数据预处理手段；
  \item 训练的特征数目；
  \item 提取特征间的步长（步幅）
  \item 感受野的大小
\end{itemize}

实验表明，无论采用什么非监督式学习算法，采用白化，大量的特征数量和小的步幅均能产生更好的性能表现。一方面，这些结果有些令人惊异。例如，业界广泛认为大量的特征表示能比采用小尺寸的表示产生更好的性能表现\cite{32}，并且对于特征间的小步幅有类似的结论\cite{21}。然而，我们工作的主要贡献就是证明这些因素对特征学习算法是否成功起到决定性作用 - 甚至比选择非监督式学习算法更重要。事实上，当我们将这些参数调节至其极限，我们可以达到迄今最好的性能表现，并且比其他更复杂的模型表现更好。我们实验的最好结果是使用k-means聚类法，这是相当出人意料的。该算法广泛使用于计算机视觉算法，但是尚未在深度特征学习中被广泛采用。具体来说，我们在CIFAR-10数据集上测试正确率为79.6\%，在NORB数据集上为97.2\%，这优于比以往发布的所有结果。

开始，我们将回顾特征学习的相关工作，接下来介绍通用的特征学习框架，我们将在第3节用它来测试并评价我们的系统。最后我们在第4节介绍我们的实验分析和展示针对CIFAR-10 \cite{13} 和NORB \cite{16}数据集上的 实验结果。
