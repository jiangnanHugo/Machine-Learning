\chapter{相关工作}
自推出无监督训练前\cite{8}，学者提出了许多新的方案，通过堆叠特征层次来获得``深层''表示。主要都集中在创建新的训练算法来建立单层模型以便组成深层模型。在历史文献中，我们主要研究的算法是稀疏编码\cite{22,17,32}，限制玻尔兹曼机\cite{8,13}，稀疏限制玻尔兹曼机\cite{30}，稀疏自动编码器\cite{7,25}，去噪自动编码器\cite{30}，分解\cite{24}和平均协方差\cite{23}限制玻尔兹曼机，以及其他算法\cite{19,33}。因此，除了特征学习构架中的许多模块，非监督式学习模块是最重要的部分。

然而，一些工作已经研究了其他选择对特征学习系统所产生影响，尤其是不同网络架构产生的影响。例如，加勒特等人研究了改变特征层次之间的池化策略和在层之间采用不同的归一化和校正手段对系统产生的改变\cite{11}。同样地，Boureau等人从理论\cite{3}和实践\cite{4}两方面研究了不同的编码策略和池化方法对系统产生的影响。我们的工作也遵循上述思路，但研究单层模型的结构，而不是池化方法，也不是不同算法和编码方案。

许多计算机视觉文献也涉及到我们的工作和涉及到特征学习算法。例如，我们将使用k-means聚类算法作为一种替代的非监督式学习算法模块。k-means在``深度学习''中使用不是很广泛，但在计算机视觉领域中是``视觉语言''的重要组成部分并被广泛采纳，他被用来刻画更高层次的图像特征\cite{5, 6, 15, 31}。这种方法也被递归地应用于构建多层特征\cite{1}。池化手段和激励函数的选定或者编码方案的不同对系统产生的影响已经被研究并讨论\cite{15,28,21}。例如，Van Gemert等人证明了``软''激励函数（``核函数''）比硬分配的激励函数在视觉词汇模型中产生的效果更好。

本文也会与前人的实验结果对照比较（例如，我们也会考虑``软''和``硬''激活函数），但是其他方面有一些不同：虽然我们可以证实，一些特征学习方案优于其他方案，我们仍展示这些所谓的不同通常因其他因素而被高估，例如特征的数量。因此，即使更复杂的学习策略可以略微优化性能，但这些可以被一些简洁但运算迅速的学习算法所替代，他们能处理更大的网络模型。
