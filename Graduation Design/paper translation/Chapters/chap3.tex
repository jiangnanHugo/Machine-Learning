\chapter{非监督特征学习框架}
在本节中，我们将描述一个用于特征学习的通用框架。具体来说，我们将集中于将这些算法应用到从图像中学习特征，尽管我们的算法也适用于其他形式的数据。我们使用的框架包含几个部分，这些部分与计算机视觉\cite{5, 15, 31, 28, 1}和特征学习\cite{16,19,3}领域算法框架十分相似。
在较高的抽象层次，我们的系统将执行以下步骤来学习特征表示：
\begin{enumerate}
  \item 从未标记训练图像中随机选取图像小块。
  \item 对这些图像小块做预处理。
  \item 利用非监督式学习算法学习特征映射函数。
\end{enumerate}

当我们获取到特征映射函数和一系列的已标记的训练图像，我们将采用特征提取和分类：
\begin{enumerate}
  \item 将输入图像划分成等间隔的小块，从中提取特征。
  \item 池化将输入图像的不同区域，以减少特征数量。
  \item 训练一个线性分类器，一预测给定特征向量的标签。
\end{enumerate}
现在，我们就将更加详细地描述这套流程和其中的参数配置。

\section{特征学习}
如上所述，系统先随机从无标记输入图像中提取子块。每一个子块尺寸大小为 $w * w$并且有$d$个通道，$w$代表``感受野的大小''。每一个$w * w$ 的子块能重排列成 $R^N$维度的图像密度向量，$N=w*w*d$。然后我们构建$m$个随机采样的小块，$X=\{x^{(1)},\cdots,x^{(m)}\}$，其中，$x^{(i)}\in R^N$。给定这样的数据集，我们接下来进行数据预处理和非监督式学习步骤。
\subsection{预处理}
通常在从数据中获取特征之前，我们需要进行简单的归一化处理步骤。在这部分工作中，我们对每一个图像小块中的$x^{(i)}$减去图像小块的平均值并除以其标准差，以实现归一化处理。针对可视化数据，这步对应于局部亮度和对比度归一化处理。

当输入向量做完归一化处理后，我们可以选择是否进行白化处理\cite{10}。虽然这个处理手段经常被用于深度学习实验中（例如，\cite{10}），但却在计算机视觉领域不常被用到。我们将对白化处理做对照实验，以决定这步骤是否是必须的。
\subsection{非监督式学习}
在预处理之后，我们采用非监督式学习算法从无标签数据中发现特征。对我们而言，我们将非监督式学习算法看作是一个``黑匣子''，它读取输入数据$X$，输出一个映射函数 $f: R^N \to R^K$， 它将输入向量映射$x^{(i)}$成$K$个特征向量，$K$是算法的参数。我们将第$k$个特征表示为$f_k$。在这项工作中，我们将使用多种不同的非监督式学习算法：
\begin{itemize}
	\item 稀疏自动编码器
	\item 稀疏限制玻尔兹曼机
	\item k-means聚类
	\item 高斯混合模型
\end{itemize}
我们将简要总结如何将这些算法应用到我们的系统中去。
\begin{enumerate}
	\item \textbf{稀疏自动编码器:}我们训练具有$K$个隐藏节点的自动编码器，使用反向传播技术来缩减平方误差与附加惩罚项，它能使节点保持一个较低平均激活值\cite{18,7}。算法输出为权重矩阵$W \in R^{K*N}$和偏置项$b\in R^K$，特征映射函数$f$定义为：
		\begin{equation}
		\label{equ:autoECD}
			f(x)=g(Wx+b)
		\end{equation}
		训练算法中使用了许多超参数（例如，权重衰减项，目标函数）。针对每一个感受野大小为$w$,我们通过交叉验证手段来调节这些参数。
	\item \textbf{稀疏限制玻尔兹曼机:}限制玻尔兹曼机(RBM)是具有$K$个二元隐含变量的无向图模型。稀疏RBMs需要加上类似于自动编码器的稀疏惩罚因子，可以利用差分逼近法\cite{9}来训练。训练结果是权重矩阵$W$和偏置项$b$，从而获得类似于自动编码器的特征映射函数(公式 \ref{equ:autoECD})――这样，这些算法的差异主要是他们的训练手段不同而已。如上所述，对于每一个不同大小的感受野尺寸，我们也采用交叉验证手段加以调节。
	\item \textbf{k-means聚类:}我们应用K-means聚类方法到输入数据来学习$K$个质心$c^{(k)}$。当获取到质心$c^{(k)}$，我们采用两种不同的特征映射函数$f$。第一种是硬分配的编码策略:
		\begin{equation}
		\label{equ:km-1}
			{f_k}(x) = \left\{ 
				\begin{array}{l}	
					\begin{array}{*{20}{c}}
						1&{if} &{k = \arg\min _j|| c^{(j)} - x||_2^2}
					\end{array}\\	
					\begin{array}{*{20}{c}}
						0&{otherwise.}
					\end{array}
				\end{array} 
				\right.
		\end{equation}
		这种稀疏表示模式已经在计算机视觉中广泛应用\cite{5}。然而，学者认为这种编码方式可能太简单了\cite{28}。因此我们的第二种特征映射函数是一个非线性函数，它能比上述编码方案更加``适合''同时也能保持稀疏度：
		\begin{equation}
		\label{km-2}
			f_k{(x)}=\max\{0,\mu(z)-z_k\}
		\end{equation}
		其中，$z_k=||x-c^{(k)}||$，$\mu(z)$代表集合$z$的平均值。对任意特征值$f_k$，当它与其质心的距离$c^{k}$距离超过平均值，该激活函数将输出0。在实践中，这意味着将近一半的特征值将被设置为0。这也可以作为特征值之间的一种简单的``竞争''。我们分别将这两种方案称作为k-means（硬）和k-means（三角）。	
	\item \textbf{高斯混合模型:} 高斯混合模型(GMMs)常用于聚类，它将输入数据的密度表示成$K$个高斯分布的聚合。该模型能利用最大期望算法（EM）来训练\cite{1}。我们将迭代一遍的k-means聚类的结果用来初始化该混合模型。映射函数$f$，将输入映射成为其后验概率:
		\begin{equation}
		\label{gmm}
		f_k(x)=\frac{1}{(2\pi)^{d/2}|\sum_k|^{1/2}}\exp (-\frac{1}{2}(x-c^{(k)})^T\sum{_k^{-1}}(x-c^{(k)}))
		\end{equation}
		其中$\sum_k$是对角协方差值，$\phi_k$是由EM算法学习得到的集群先验概率。
		
\end{enumerate}
\section{特征提取和分类}
对特定的非监督式学习算法来说，上述步骤会产出一个函数$f$，他会将输入小块 $x\in R^N$转换成新的表示形式$y=f(x)\in R^K$。我们将利用这个特征提取器从我们有标签的训练图像中获取特征并进行分类。
\subsection{卷积提取}
对任意给定$w*w$大小的图像小块，利用学习到的特征提取器$f:R^N \to R^K$，我们可以计算获得它新的表示形式$y \in R^K$。通过该步骤，我们可以获得整个图像的单层表示形式。具体地来说，给定一个$n*n$像素大小(有$d$个通道)的图片，我们计算每一个大小为$w*w$的子块，获得了$(n-w+1)*(n-w+1)$大小的特征表示(有$K$个通道)。形式上说，我们令$y^{(ij)}$为输入图像中以坐标$(i,j)$开始的大小为$w*w$的第$K$通道的图像小块的新特征表示。考虑到计算上的效率，我们也可以调节特征提取器的步幅，令其大于1。在图\ref{fig:conv}展示了上述实验步骤。
\begin{figure}
\centering
\includegraphics[scale=0.7]{./Pictures/conv.eps}
\caption{白化的效果和质心的数量}
\label{fig:conv}
\end{figure}
\subsection{分类}
在分类之前，池化方法能有效减少图像的维度。若步幅设置为$s=1$，我们通过特征映射就能获得$(n-w+1)*(n-w+1)*K$大小的特征表示。我们可以把同为$y^{(ij)}$的区域都相加起来，以进一步缩减维度。这样的操作在计算机视觉领域\cite{15}和深度学习领域\cite{11}都广泛应用。在我们的系统中，我们采用十分简洁形式的池化方法。具体来说，我们将$y^{(ij)}$划分成四个相等大小的象限。并分别计算每个$y^{(ij)}$在其上的总和。对于每个象限，我们实现了维度规约，我们将数量为$4K$的特征用来分类。

